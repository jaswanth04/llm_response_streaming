{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TextStreamer\n",
    "import torch\n",
    "from queue import Queue\n",
    "from threading import Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-30 11:15:33,063] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8805cacba17427f9d1fd56fcdd9b576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True, \n",
    "                                            bnb_4bit_quant_type='nf4',\n",
    "                                            bnb_4bit_use_double_quant=True,\n",
    "                                            bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "device = \"cuda:0\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        trust_remote_code=True,\n",
    "        # config=model_config,\n",
    "        quantization_config=bnb_config,\n",
    "        torch_dtype=torch.float16,\n",
    "        # load_in_8bit=True,\n",
    "        device_map=device,\n",
    "        # use_auth_token=hf_auth\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_new_tokens = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_inst_tags(input_string):\n",
    "    while '[INST]' in input_string:\n",
    "        start_index = input_string.find('[INST]')\n",
    "        end_index = input_string.rfind('[/INST]') + len('[/INST]')\n",
    "        input_string = input_string[:start_index] + input_string[end_index:]\n",
    "    return input_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomStreamer(TextStreamer):\n",
    "\n",
    "    def __init__(self, queue,tokenizer, skip_prompt,**decode_kwargs) -> None:\n",
    "        super().__init__(tokenizer, skip_prompt, **decode_kwargs)\n",
    "        self._queue = queue\n",
    "        self.stop_signal=None\n",
    "        self.timeout = 1\n",
    "        \n",
    "    def on_finalized_text(self, text: str, stream_end: bool = False):\n",
    "        self._queue.put(text)\n",
    "        if stream_end:\n",
    "            self._queue.put(self.stop_signal)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hi, my name is Jaswant\"\n",
    "\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": text\n",
    "}]\n",
    "inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "streamer_queue = Queue()\n",
    "streamer = CustomStreamer(streamer_queue, tokenizer, True)\n",
    "generation_kwargs = dict(inputs=inputs, streamer=streamer, \n",
    "                         pad_token_id=tokenizer.eos_token_id, \n",
    "                         max_new_tokens=64, temperature=0.1,\n",
    "                         do_sample=True)\n",
    "\n",
    "# outputs = model.generate(inputs, max_new_tokens=max_new_tokens, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "# output = remove_inst_tags(decoded_output)\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Jaswant, nice to meet you! How can I help you today? If you have any specific questions or topics you'd like to discuss, feel free to ask. I'm here to provide information and answer any questions you might have to the best of my ability. Let me know if there'"
     ]
    }
   ],
   "source": [
    "thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "thread.start()\n",
    "\n",
    "while True:\n",
    "    value = streamer_queue.get()\n",
    "    if value == None:\n",
    "        break\n",
    "    print(value, end=\"\")\n",
    "    streamer_queue.task_done()\n",
    "    # await asyncio.sleep(0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Hello Jaswant, nice to meet you! How can I help you today? If you have any specific questions or topics you'd like to discuss, feel free to ask. I'm here to provide information and answer any questions you might have to the best of my ability. Let me know if there'\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model.generate(inputs=inputs, streamer=streamer, \n",
    "                         pad_token_id=tokenizer.eos_token_id, max_new_tokens=64, temperature=0.1, do_sample=True)\n",
    "\n",
    "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "output = remove_inst_tags(decoded_output)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer_queue = Queue()\n",
    "streamer = CustomStreamer(streamer_queue, tokenizer, True)\n",
    "\n",
    "def generate(messages):\n",
    "    inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    outputs = model.generate(inputs=inputs, streamer=streamer, \n",
    "                         pad_token_id=tokenizer.eos_token_id, \n",
    "                         max_new_tokens=max_new_tokens, \n",
    "                         temperature=0.1, do_sample=True)\n",
    "\n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    output = remove_inst_tags(decoded_output)\n",
    "    print('\\n----------Displaying Output-------------\\n')\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To print the text \"Hello World\" in Python, you can use the `print()` function. Here's an example of how to use it:\n",
      "\n",
      "```python\n",
      "# This is a comment - it will not be executed\n",
      "\n",
      "# Use the print function to print \"Hello World\"\n",
      "print(\"Hello World\")\n",
      "```\n",
      "\n",
      "When you run this code, the output will be:\n",
      "\n",
      "```\n",
      "Hello World\n",
      "```\n",
      "\n",
      "You can also print multiple lines by separating them with commas:\n",
      "\n",
      "```python\n",
      "print(\"Hello World\", \"This is a second line\")\n",
      "```\n",
      "\n",
      "The output will be:\n",
      "\n",
      "```\n",
      "Hello World\n",
      "This is a second line\n",
      "```</s>\n",
      "----------Displaying Output-------------\n",
      "\n",
      " To print the text \"Hello World\" in Python, you can use the `print()` function. Here's an example of how to use it:\n",
      "\n",
      "```python\n",
      "# This is a comment - it will not be executed\n",
      "\n",
      "# Use the print function to print \"Hello World\"\n",
      "print(\"Hello World\")\n",
      "```\n",
      "\n",
      "When you run this code, the output will be:\n",
      "\n",
      "```\n",
      "Hello World\n",
      "```\n",
      "\n",
      "You can also print multiple lines by separating them with commas:\n",
      "\n",
      "```python\n",
      "print(\"Hello World\", \"This is a second line\")\n",
      "```\n",
      "\n",
      "The output will be:\n",
      "\n",
      "```\n",
      "Hello World\n",
      "This is a second line\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "text = \"How to print Hello World in python ?\"\n",
    "\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": text\n",
    "}]\n",
    "\n",
    "thread = Thread(target=generate, kwargs=dict(messages=messages))\n",
    "thread.start()\n",
    "\n",
    "while True:\n",
    "    value = streamer_queue.get()\n",
    "    if value == None:\n",
    "        break\n",
    "    print(value, end=\"\")\n",
    "    streamer_queue.task_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
